{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpus = []\\nf = open(\\'alice_in_wonderland.txt\\',\\'r\\')\\nwhile(1):\\n    line =  f.readline()\\n    if len(line) == 0: break\\n    corpus.extend(line.split())\\n        \\nf.close()\\n\\n\\ndef clean_word(word):\\n    word = word.lower().strip()\\n    for punctuation in [\\'*\\',\\'\"\\',\"\\'\",\\'.\\',\\',\\',\\'-\\',\\'?\\',\\'!\\',\\';\\',\\':\\',\\'—\\',\\'(\\',\\')\\',\\'[\\',\\']\\']:\\n        \\n        word = \\'\\'.join(word.split(punctuation))\\n    \\n    return word\\n\\ncorpus = [clean_word(word) for word in corpus]\\ncorpus = [word for word in corpus if len(word) > 0]\\n\\ncorrupted_corpus = copy.deepcopy(corpus)\\n\\np = .25\\nalphabet = \\'abcdefghijklmnopqrstuvwxyz\\'\\nfor k in xrange(len(corrupted_corpus)):\\n    word = corrupted_corpus[k]\\n    if len(word) < 2: continue\\n    if np.random.rand() < p:\\n        if np.random.randn() < 0:\\n            swap = np.random.choice(range(len(word)), size=2, replace = False)\\n            swap = np.sort(swap)\\n            word = \\'\\'.join([word[:swap[0]], word[swap[1]], word[swap[0]+1:swap[1]], word[swap[0]], word[swap[1]+1:]])\\n        else:\\n            \\n            replace = np.random.choice(range(len(word)), size=1, replace = False)[0]\\n            replace_with = alphabet[np.random.choice(range(len(alphabet)),size=1)[0]]\\n            word = \\'\\'.join([word[:replace], replace_with, word[replace+1:]])\\n        \\n        corrupted_corpus[k] = word\\n\\npickle.dump({\\'corpus\\':corpus,\\'corrupted_corpus\\':corrupted_corpus},open(\\'alice_spelling.pkl\\',\\'wb\\'))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This was the process used to load and clean the corpus, and produce the corrupted corpus.\n",
    "#You don't need to do anything here.\n",
    "\"\"\"\n",
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower().strip()\n",
    "    for punctuation in ['*','\"',\"'\",'.',',','-','?','!',';',':','—','(',')','[',']']:\n",
    "        \n",
    "        word = ''.join(word.split(punctuation))\n",
    "    \n",
    "    return word\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "\n",
    "corrupted_corpus = copy.deepcopy(corpus)\n",
    "\n",
    "p = .25\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "for k in xrange(len(corrupted_corpus)):\n",
    "    word = corrupted_corpus[k]\n",
    "    if len(word) < 2: continue\n",
    "    if np.random.rand() < p:\n",
    "        if np.random.randn() < 0:\n",
    "            swap = np.random.choice(range(len(word)), size=2, replace = False)\n",
    "            swap = np.sort(swap)\n",
    "            word = ''.join([word[:swap[0]], word[swap[1]], word[swap[0]+1:swap[1]], word[swap[0]], word[swap[1]+1:]])\n",
    "        else:\n",
    "            \n",
    "            replace = np.random.choice(range(len(word)), size=1, replace = False)[0]\n",
    "            replace_with = alphabet[np.random.choice(range(len(alphabet)),size=1)[0]]\n",
    "            word = ''.join([word[:replace], replace_with, word[replace+1:]])\n",
    "        \n",
    "        corrupted_corpus[k] = word\n",
    "\n",
    "pickle.dump({'corpus':corpus,'corrupted_corpus':corrupted_corpus},open('alice_spelling.pkl','wb'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current recovery rate 0.7696784189245361\n",
      "prob misspelling alice vs alace 0.8\n",
      "prob misspelling alice vs earth 1e-06\n",
      "prob misspelling machinelearning vs machinedreaming 0.6666666666666666\n",
      "prob misspelling machinelearning vs artificalintell 1e-06\n"
     ]
    }
   ],
   "source": [
    "#Take a look at how the data looks, and let's make some helper functions.\n",
    "data = pickle.load(open('alice_spelling.pkl','rb'))\n",
    "vocab = np.unique(data['corpus'])\n",
    "V = len(vocab)\n",
    "\n",
    "\n",
    "## CORRECT VS INCORRECT CORPUS\n",
    "##For now, we will hold onto both the correct and incorrect corpuses. Later, you will only process the incorrect corpus, and the correct corpus is only used as a reference to check for recovery accuracy.\n",
    "def recovery_rate(new_corpus, correct_corpus):\n",
    "    wrong = 0\n",
    "    for k in range(len(new_corpus)):\n",
    "        if new_corpus[k] != correct_corpus[k]:\n",
    "            wrong += 1\n",
    "    return 1.- wrong/(len(new_corpus)+0.)\n",
    "print('current recovery rate', recovery_rate(data['corpus'],data['corrupted_corpus'] ))\n",
    "\n",
    "## Probability of a word mispelling\n",
    "## We will use the following function to predict whether a misspelled word was actually another word. To avoid numerical issues, we make sure that the probablity is always something nonzero.\n",
    "def prob_misspelled(word1,word2):\n",
    "    SMALLNUM = 0.000001\n",
    "    if len(word1) != len(word2): return SMALLNUM\n",
    "    num_wrong = np.sum(np.array([word1[k] == word2[k] for k in range(len(word1))]))\n",
    "    return np.maximum(num_wrong / (len(word1) + 0.),SMALLNUM)\n",
    "print('prob misspelling alice vs alace', prob_misspelled('alice','alace'))\n",
    "print('prob misspelling alice vs earth', prob_misspelled('alice','earth'))\n",
    "print('prob misspelling machinelearning vs machinedreaming', prob_misspelled('machinelearning','machinedreaming'))\n",
    "print('prob misspelling machinelearning vs artificalintell', prob_misspelled('machinelearning','artificalintell'))\n",
    "\n",
    "##HASHING\n",
    "#all of our objects should be vectors of length V or matrices which are V x V. \n",
    "#the kth word in the vocab list is represented by the kth element of the vector, and the relationship between the i,jth words is represented in the i,jth element in the matrix.\n",
    "# to easily go between the word indices and words themselves, we need to make a hash table. \n",
    "vocab_hash = {}\n",
    "for k in range(len(vocab)):\n",
    "    vocab_hash[vocab[k]] = k\n",
    "    \n",
    "#now, to access the $k$th word, we do vocab[k]. To access the index of a word, we call vocab_hash[word].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob. of \"alice\" 0.014548615047424706\n",
      "prob. of \"queen\" 0.002569625514869818\n",
      "prob. of \"chapter\" 0.0009069266523069947\n"
     ]
    }
   ],
   "source": [
    "## FILL ME IN ##\n",
    "\n",
    "#WORD FREQUENCY\n",
    "#create an array of length V where V[k] returns the normalized frequency of word k in the entire data corpus. Do so by filling in this function.\n",
    "def get_word_freq(corpus):\n",
    "    word_freq = np.zeros((len(vocab)))\n",
    "                         \n",
    "    for i in range(len(vocab)-1):\n",
    "        freq = 0\n",
    "        for j in range(len(corpus)-1):\n",
    "            if vocab[i] == corpus[j]:\n",
    "                word_freq[i]+=1\n",
    "            \n",
    "        \n",
    "    return  np.array(word_freq)\n",
    "    \n",
    "#create an array of length V where V[k] returns the normalized frequency of word k in the entire data corpus. Do so by filling in this function.\n",
    "word_freq =  get_word_freq(data['corpus'])\n",
    "\n",
    "def get_word_prob(corpus):\n",
    "    word_prob = []\n",
    "    for i in range(len(vocab)-1):\n",
    "        x = word_freq[i] / len(corpus)\n",
    "        word_prob.append(x)\n",
    "\n",
    "\n",
    "        \n",
    "    return np.array(word_prob)\n",
    "word_prob =  get_word_prob(data['corpus'])\n",
    "\n",
    "#report the answer of the following:\n",
    "print('prob. of \"alice\"', word_prob[vocab_hash['alice']])\n",
    "print('prob. of \"queen\"', word_prob[vocab_hash['queen']])\n",
    "print('prob. of \"chapter\"', word_prob[vocab_hash['chapter']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob. of \"the alice\" 0.0\n",
      "prob. of \"the queen\" 0.03970678069639585\n",
      "prob. of \"the chapter\" 0.0\n",
      "prob. of \"the hatter\" 0.031154551007941355\n"
     ]
    }
   ],
   "source": [
    "# Pr(word | prev word) \n",
    "#Using the uncorrupted corpus, accumulate the conditional transition probabilities. \n",
    "def get_transition_matrix(corpus):\n",
    "    count_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    transition_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    \n",
    "    for i in range(1,len(corpus)-1):\n",
    "        token_i = vocab_hash[corpus[i]]\n",
    "        token_im1 = vocab_hash[corpus[i-1]]\n",
    "        transition_matrix[token_i,token_im1] += 1\n",
    "    \n",
    "    for i in range(len(vocab)-1):\n",
    "        for j in range(len(vocab)-1):\n",
    "            transition_matrix[i,j] =  ((transition_matrix[i,j])/ 26463)/word_prob[j]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "    return transition_matrix\n",
    "transition_matrix = get_transition_matrix(data['corpus'])\n",
    "print('prob. of \"the alice\"', transition_matrix[vocab_hash['alice'],vocab_hash['the']])\n",
    "print('prob. of \"the queen\"', transition_matrix[vocab_hash['queen'],vocab_hash['the']])\n",
    "print('prob. of \"the chapter\"', transition_matrix[vocab_hash['chapter'],vocab_hash['the']])\n",
    "\n",
    "print('prob. of \"the hatter\"', transition_matrix[vocab_hash['hatter'],vocab_hash['the']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abide', 'alice', 'above', 'twice', 'alive', 'voice', 'which', 'white', 'agree', 'write']\n"
     ]
    }
   ],
   "source": [
    "#The prior probabilities are just the word frequencies\n",
    "prior = word_prob  + 0.\n",
    "\n",
    "#write a function that returns the emission probability of a potentially misspelled word, by comparing its probabilities against every word in the correct vocabulary\n",
    "def get_emission(mword):\n",
    "    emmision = []\n",
    "    for i in range(len(vocab)-1):\n",
    "        emmision.append(prob_misspelled(mword,vocab[i]))\n",
    "\n",
    "\n",
    "    return emmision\n",
    "\n",
    "#find the 10 closest words to 'abice' and report them\n",
    "idx = np.argsort(get_emission('abice'))[::-1]\n",
    "print([vocab[j] for j in idx[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we reduce our attention to a small segment of the corrupted corpus\n",
    "corrupt_corpus =   data['corrupted_corpus'][:1000]\n",
    "correct_corpus =   data['corpus'][:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/0wtklwy91_sbdqn9nv18dqhh0000gn/T/ipykernel_1903/91367736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupt_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupt_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransition_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrect_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mforward_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "# encode the HMM spelling corrector. To debug, you can see the first hundred words of both the corrupted and proposed corpus, to see if spelling words got corrupted.\n",
    "\n",
    "# report the recovery rate of the proposed (corrected) corpus.\n",
    "\n",
    " \n",
    "#forward step\n",
    "forward_probs = np.ones((len(corrupt_corpus), V))/(V+0.)\n",
    "for i in range(len(corrupt_corpus)-1):\n",
    "    for j in range(len(correct_corpus)-1):\n",
    "        x = get_emission(corrupt_corpus[i]) * transition_matrix[vocab_hash[correct_corpus[i]],j]\n",
    "        if i != 0:\n",
    "            x = x * forward_probs[i-1]\n",
    "    for k in range(len(forward_probs)-1):\n",
    "        forward_probs[i,k] = x[k]\n",
    "\n",
    "    \n",
    "# backward step\n",
    "backward_probs = np.ones((len(corrupt_corpus), V))/(V+0.)\n",
    "for i in range(len(corrupt_corpus)-1, 0, -1):\n",
    "    for j in range(len(correct_corpus)-1, 0, -1):\n",
    "        x = get_emission(corrupt_corpus[i]) * transition_matrix[vocab_hash[correct_corpus[i]],j]\n",
    "        if i != 0:\n",
    "            x = x * backward_probs[i-1]\n",
    "    for k in range(len(forward_probs)-1):\n",
    "        backward_probs[i,k] = x[k]\n",
    "\n",
    "\n",
    "#return likelihood of kth word in corpus   \n",
    "def get_max_likelihood(k):\n",
    "    return np.max( forward_probs[k] * backward_probs[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_max_likelihood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/0wtklwy91_sbdqn9nv18dqhh0000gn/T/ipykernel_1903/1459792034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mproposed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupt_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupt_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_max_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mproposed_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_max_likelihood' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "proposed_corpus = copy.deepcopy(corrupt_corpus)\n",
    "for k in range(len(corrupt_corpus)):\n",
    "    new_word = vocab[get_max_likelihood(k)]\n",
    "    proposed_corpus[k] = new_word\n",
    "\n",
    "print(recovery_rate(corrupt_corpus, correct_corpus), recovery_rate(proposed_corpus, correct_corpus) )\n",
    "for trial in range(100):\n",
    "    k = int(np.random.rand()*1000)\n",
    "    if correct_corpus[k] == corrupt_corpus[k]: continue\n",
    "    print(correct_corpus[k],corrupt_corpus[k],proposed_corpus[k])\n",
    "    j = np.argmax(forward_probs[k,:]*backward_probs[k,:])\n",
    "    i = vocab_hash[correct_corpus[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
